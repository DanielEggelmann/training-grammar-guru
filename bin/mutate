#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# Copyright 2017 Eddie Antonio Santos <easantos@ualberta.ca>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
Mutate and predict on one fold.

Does not **evaluate**, simply mutates files and performs predictions.

This evaluation:
 - is intended to test the algorithm given a number of different scenarios
 - demonstrates theoretical efficacy
 - is not representative of "real-life" errors

"""

# TODO: Delete all of the stuff replaced by SourceFile, TokenSequence, Edit
# TODO: Create dedicated prediction storage.
# TODO: Create dedicated mutation storage (use Edit#serialize())
# TODO: serialize predictions (context AND result) using array.array.

import argparse
import array
import functools
import io
import random
import sqlite3
import sys
import tempfile

from pathlib import Path
from typing import Sequence

from tqdm import tqdm

from sensibility import (
    Vectors, Corpus, Vind, vocabulary,
    SourceFile,
    Edit, Insertion, Deletion, Substitution
)
from sensibility.model import Model
from sensibility._paths import (
    MODEL_DIR,
    VECTORS_PATH, SOURCES_PATH,
    PREDICTIONS_PATH, MUTATIONS_PATH
)

# According to Campbell et al. 2014
MAX_MUTATIONS = 120

# Set up the argument parser
parser = argparse.ArgumentParser()
parser.add_argument('fold', type=int)
parser.add_argument('test_set', type=Path)
parser.add_argument('-k', '--mutations', type=int, default=MAX_MUTATIONS)
parser.add_argument('-n', '--limit', type=int, default=None)
parser.add_argument('-o', '--offset', type=int, default=0)

# Schema for the results database.
SCHEMA = r"""
PRAGMA encoding = "UTF-8";

CREATE TABLE IF NOT EXISTS mutant (
    hash        TEXT NOT NULL,  -- file hash
    type        TEXT NOT NULL,  -- 'addition', 'deletion', or 'substitution'
    location    INTEGER,        -- location in the file (0-indexed)
    token       INTEGER,        -- addition: the token inserted
                                -- deletion: the token deleted
                                -- substitution: the token that has replaced the old token

    PRIMARY KEY (hash, type, location, token)
);

CREATE TABLE IF NOT EXISTS prediction (
    model   TEXT NOT NULL,      -- model that created the prediction
    context BLOB NOT NULL,      -- input of the prediction

    data    BLOB NOT NULL,      -- prediction data, as a numpy array

    PRIMARY KEY (model, context)
);

-- same as `mutant`, but contains syntactically-correct mutants.
CREATE TABLE IF NOT EXISTS correct_mutant (
    hash        TEXT NOT NULL,
    type        TEXT NOT NULL,
    location    INTEGER,
    token       INTEGER,

    PRIMARY KEY (hash, type, location, token)
);
"""


class Predictions:
    """
    Stores predictions.
    """
    def __init__(self, fold: int, filename: Path=PREDICTIONS_PATH) -> None:
        self.forwards_model = Model(MODEL_DIR / f"javascript-f{fold}.hdf5")
        self.forwards_model = Model(MODEL_DIR / f"javascript-b{fold}.hdf5",
                                    backwards=True)
        # XXX: Hard code the context length!
        self.context_length = 20

        def _predict(model_recipe, model, tuple_context):
            """
            Does prediction, consulting the database first before consulting
            the model.
            """
            context = array.array(tuple_context, np.uint8)
            stashed_prediction = self.persistence.get_prediction(
                    model_recipe=model_recipe,
                    context=context
            )
            if stashed_prediction is None:
                prediction = model.predict(context)
                self.persistence.add_prediction(
                        model_recipe=model_recipe,
                        context=context,
                        prediction=prediction)
                return prediction
            return stashed_prediction

        # Create cached prediction functions.
        @functools.lru_cache(maxsize=2**16)
        def predict_forwards(prefix):
            return _predict(forwards, self.forwards_model, prefix)

        @functools.lru_cache(maxsize=2**16)
        def predict_backwards(suffix):
            return _predict(backwards, self.backwards_model, suffix)

        self.predict_forwards = predict_forwards
        self.predict_backwards = predict_backwards

    def predict(self, filename):
        """
        Predicts at each position in the file.

        Side-effect: writes predictions to persistence.
        """

        # Get file vector for this (incorrect) file.
        with open(str(filename), 'rt', encoding='UTF-8') as script:
            tokens = tokenize_file(script)
        file_vector = vectorize_tokens(tokens)

        # Create predictions.
        for (prefix, _), (suffix, _) in self.contexts(file_vector):
            self.predict_forwards(tuple(prefix))
            self.predict_backwards(tuple(suffix))

    def clear_cache(self):
        self.predict_forwards.cache_clear()
        self.predict_backwards.cache_clear()

    def cache_info(self):
        return self.predict_forwards.cache_info(), self.predict_backwards.cache_info()

    def contexts(self, file_vector):
        """
        Yield every context (prefix, suffix) in the given file vector.
        """
        sent_forwards = Sentences(file_vector,
                                  size=self.context_length,
                                  backwards=False)
        sent_backwards = Sentences(file_vector,
                                   size=self.context_length,
                                   backwards=True)
        return zip(sent_forwards, chop_prefix(sent_backwards))

    @staticmethod
    def is_okay(filename):
        """
        Check if the syntax is okay.
        """
        with open(filename, 'rb') as source_file:
            return check_syntax_file(source_file)


def serialize_context(context: Sequence[Vind]) -> array.array:
    """
    Convert context to an unsigned 8-bit array.array.

    >>> serialize_context([32, 67, 23, 99])
    array('B', [32, 67, 23, 99])
    """
    return array.array('B', context)


class Mutations:
    """
    Persist every mutation, and enough data to reconstruct every single
    prediction.
    """

    def __init__(self, database=MUTATIONS_PATH):
        self._program = None
        self._dbname = database
        self._conn = None

    def __len__(self):
        assert self._conn
        return self._conn.execute(r'''
            SELECT COUNT(*) FROM mutant
        ''').fetchall()[0][0]

    def __iter__(self):
        assert self._conn
        cur = self._conn.execute(r'''
            SELECT hash, type, location, token FROM mutant
        ''')
        for file_hash, _type, location, token in cur:
            mutation = constructor[_type](int(location), int(token) if token else token)
            yield file_hash, mutation

    @property
    def program(self):
        """
        SourceCode object that is currently being mutated.
        """
        return self._program

    @program.setter
    def program(self, new_program):
        assert isinstance(new_program, SourceCode)
        self._program = new_program

    @property
    def current_source_hash(self):
        """
        hash of the current source file being mutated.
        """
        return self._program.hash

    def add_mutant(self, mutation):
        """
        Register a new complete mutation.
        """
        return self._add_mutant(mutation, usable_mutation=True)

    def add_correct_file(self, mutation):
        """
        Records that a mutation created a syntactically-correct file.
        """
        return self._add_mutant(mutation, usable_mutation=False)

    def _add_mutant(self, mutation, *, usable_mutation=None):
        assert self._conn
        assert isinstance(mutation, Mutation)

        if usable_mutation:
            sql = r'''
                INSERT INTO mutant(hash, type, location, token)
                     VALUES (:hash, :type, :location, :token)
            '''
        else:
            sql = r'''
                INSERT INTO correct_mutant(hash, type, location, token)
                     VALUES (:hash, :type, :location, :token)
            '''

        with self._conn:
            self._conn.execute(sql, dict(hash=self.current_source_hash,
                                         type=mutation.name,
                                         location=mutation.location,
                                         token=mutation.token))

    def add_prediction(self, *, model_recipe=None, context=None,
                       prediction=None):
        """
        Add the prediction (model, context) -> prediction
        """
        assert self._conn
        assert isinstance(model_recipe, ModelRecipe)
        assert isinstance(prediction, np.ndarray)

        with self._conn:
            self._conn.execute('BEGIN')
            self._conn.execute(r'''
                INSERT INTO prediction(model, context, data)
                VALUES (:model, :context, :data)
            ''', dict(model=model_recipe.identifier,
                      context=serialize_context(context),
                      data=to_blob(prediction)))

    def get_prediction(self, *, model_recipe=None, context=None):
        """
        Try to fetch the prediction from the database.

        Returns None if the entry is not found.
        """
        assert self._conn
        assert isinstance(model_recipe, ModelRecipe)
        cur = self._conn.execute(r'''
            SELECT data
            FROM prediction
            WHERE model = :model AND context = :context
        ''', dict(model=model_recipe.identifier,
                  context=serialize_context(context)))
        result = cur.fetchall()

        if not result:
            # Prediction not found!
            return None
        else:
            # Return the precomputed prediction.
            return unblob(result[0][0])

    def __enter__(self):
        # Connect to the database
        conn = self._conn = sqlite3.connect(str(self._dbname))
        # Initialize the database.
        with conn:
            conn.executescript(SCHEMA)
            # Some speed optimizations:
            # http://codificar.com.br/blog/sqlite-optimization-faq/
            conn.executescript(r'''
                PRAGMA journal_mode = WAL;
                PRAGMA synchronous = normal;
            ''')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._conn.close()
        self._conn = self._program = None


def test() -> None:
    """
    Test source code mutations.
    """

    # The token stream INCLUDES start and stop tokens.
    # TODO: Fix these tests to make up for 20 token "margin"

    """
    program = SourceCode('DEADBEEF', [0, 86, 5, 31, 99])
    a = Addition.create_random_mutation(program)
    b = Addition.create_random_mutation(program)
    c = Addition.create_random_mutation(program)
    assert len({a, b, c, c, b, a, c, b}) == 3

    d1 = Addition(1, 1)
    d2 = Addition(1, 1)
    s = Substitution(1, 1)
    assert d1 == d2
    assert s != d1

    mutation = Addition.create_random_mutation(program)
    mutation.format(program)

    p = Persistence(database=':memory:')

    original_model = ModelRecipe.from_string('python-f-310-5.4.5.h5')
    alternate_model = ModelRecipe.from_string('python-b-310-5.4.5.h5')
    sentence = (0, 10, 12, 67, 32)
    context = np.array(sentence, np.uint8)
    predictions = np.array([random.random() for _ in range(100)])

    with p:
        # Initially, it should return None.
        pred = p.get_prediction(model_recipe=original_model, context=context)
        assert pred is None

        p.add_prediction(model_recipe=original_model, context=context,
                         prediction=predictions)

        # Get the new prediction.
        result = p.get_prediction(model_recipe=original_model,
                                  context=context)
        assert result is not None
        assert all(x == y for x, y in zip(predictions, result))
        # Ensure the other context is not the same.
        assert p.get_prediction(model_recipe=alternate_model,
                                context=context) is None
    """


def main() -> None:
    # Requires: corpus, model data (backwards and forwards)
    args = parser.parse_args()

    CONTEXT_LENGTH = 20
    SENTENCE_LENGTH = CONTEXT_LENGTH + 1
    MIN_LENGTH = SENTENCE_LENGTH * 2

    test_set_filename: Path = args.test_set
    limit: int = args.limit
    offset: int = args.offset

    SourceFile.vectors = Vectors.connect_to(VECTORS_PATH)
    SourceFile.corpus = Corpus.connect_to(SOURCES_PATH)

    # Loads the parallel models.
    sensibility = Predictions(args.fold)

    # Load the test set. Assume the file hashes are already in random order.
    with open(str(test_set_filename)) as test_set_file:
        test_set = tuple(line.strip() for line in test_set_file
                         if line.strip())

    # Resize the test set
    upper_bound = offset + limit if limit is not None else len(test_set)
    test_set = test_set[offset:upper_bound]

    print(f"Considering {len(test_set)} test files to mutate...")

    with Mutations() as persist:
        # Mutate each file in the test set.
        for file_hash in tqdm(test_set):
            # Get the file
            program = SourceFile(file_hash)

            # Ignore files that are too short.
            if len(program.vector) < 42:
                continue

            persist.program = program
            progress = tqdm(total=args.mutations * 3, leave=False)

            for mutation_kind in Insertion, Deletion, Substitution:
                failures = 0
                mutations_seen = set()

                # Clamp down the maximum number of mutations.
                max_mutations = min(args.mutations, program.usable_length)
                max_failures = max_mutations

                while failures < max_failures and len(mutations_seen) < max_mutations:
                    mutation = mutation_kind.create_random_mutation(program)
                    if mutation in mutations_seen:
                        failures += 1
                        continue
                    mutations_seen.add(mutation)

                    # Write out the mutated file.
                    with tempfile.NamedTemporaryFile(mode='w+t', encoding='UTF-8') as mutated_file:
                        # Apply the mutatation and write it to disk.
                        mutation.format(program, mutated_file)
                        mutated_file.flush()

                        # Try the file, reject if it compiles.
                        if sensibility.is_okay(mutated_file.name):
                            persist.add_correct_file(mutation)
                            failures += 1
                            continue

                        # Do it!
                        sensibility.predict(mutated_file.name)

                    persist.add_mutant(mutation)
                    progress.update(1)

                    # Update the description.
                    progress.set_description(
                        f'{mutation_kind.code} {len(mutations_seen):d}({failures})'
                    )

            # Close the tqdm progress bar.
            progress.close()
            # Clear the LRU cache for the new file.
            sensibility.clear_cache()


if __name__ == '__main__':
    main()
