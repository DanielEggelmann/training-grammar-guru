#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

# Copyright 2017 Eddie Antonio Santos <easantos@ualberta.ca>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
Mutate and predict on one fold.

Does not **evaluate**, simply mutates files and performs predictions.

This evaluation:
 - is intended to test the algorithm given a number of different scenarios
 - demonstrates theoretical efficacy
 - is not representative of "real-life" errors

"""

import argparse
import array
import functools
import io
import random
import sqlite3
import sys
from tempfile import NamedTemporaryFile

from pathlib import Path
from typing import (
    Iterator, Iterable,
    Sequence, Set, Sized, Tuple,
    Optional, Type, cast, TextIO,
    Union
)

from tqdm import tqdm
import numpy as np

from sensibility import (
    Vectors, Corpus, Vind, vocabulary,
    SourceFile,
    Edit, Insertion, Deletion, Substitution,
    vectorize_tokens,
    Sentence, forward_sentences, backward_sentences
)
from sensibility.model import Model
from sensibility._paths import (
    MODEL_DIR,
    VECTORS_PATH, SOURCES_PATH,
    PREDICTIONS_PATH, MUTATIONS_PATH
)
from sensibility.abram import at_most
from sensibility.tokenize_js import check_syntax_file, tokenize_file

from sensibility.sentences import T
Contexts = Iterable[Tuple[Sentence[T], Sentence[T]]]


# According to Campbell et al. 2014
MAX_MUTATIONS = 120

# Set up the argument parser
parser = argparse.ArgumentParser()
parser.add_argument('fold', type=int)
parser.add_argument('test_set', type=Path)
parser.add_argument('-k', '--mutations', type=int, default=MAX_MUTATIONS)
parser.add_argument('-n', '--limit', type=int, default=None)
parser.add_argument('-o', '--offset', type=int, default=0)


class Predictions:
    """
    Stores predictions.
    """

    SCHEMA = r"""
    PRAGMA encoding = "UTF-8";

    CREATE TABLE IF NOT EXISTS prediction (
        model   TEXT NOT NULL,      -- model that created the prediction
        context BLOB NOT NULL,      -- input of the prediction, as an array
        vector  BLOB NOT NULL,      -- prediction data, as an array

        PRIMARY KEY (model, context)
    );
    """

    def __init__(self, fold: int, filename: Path=PREDICTIONS_PATH) -> None:
        assert 0 <= fold < 5
        self.forwards_model = Model(MODEL_DIR / f"javascript-f{fold}.hdf5")
        self.backwards_model = Model(MODEL_DIR / f"javascript-b{fold}.hdf5",
                                     backwards=True)
        # XXX: Hard code the context length!
        self.context_length = 20
        self._conn = sqlite3.connect(str(filename))

        forwards = f'f{fold}'
        backwards = f'b{fold}'

        def _predict(name: str, model: Model,
                     tuple_context: Sequence[Vind]) -> array.array:
            """
            Does prediction, consulting the database first before consulting
            the model.
            """
            context = array.array('B', tuple_context).tobytes()
            try:
                return self.get_prediction(name, context)
            except KeyError:
                prediction = model.predict(tuple_context)
                self.add_prediction(name, context, prediction)
                return prediction

        # Create cached prediction functions.
        @functools.lru_cache(maxsize=2**16)
        def predict_forwards(prefix: Tuple[Vind, ...]) -> None:
            _predict(forwards, self.forwards_model, prefix)

        @functools.lru_cache(maxsize=2**16)
        def predict_backwards(suffix: Tuple[Vind, ...]) -> None:
            _predict(backwards, self.backwards_model, suffix)

        self.predict_forwards = predict_forwards
        self.predict_backwards = predict_backwards

    def predict(self, filename: Union[Path, str]) -> None:
        """
        Predicts at each position in the file, writing predictions to
        database.
        """

        # Get file vector for this (incorrect) file.
        with open(str(filename), 'rt', encoding='UTF-8') as script:
            tokens = tokenize_file(cast(TextIO, script))
        file_vector = vectorize_tokens(tokens)

        # Calculate and store predictions.
        for (prefix, _), (suffix, _) in self.contexts(file_vector):
            self.predict_forwards(tuple(prefix))
            self.predict_backwards(tuple(suffix))

    def clear_cache(self):
        self.predict_forwards.cache_clear()
        self.predict_backwards.cache_clear()

    def contexts(self, file_vector: Sequence[Vind]) -> Contexts:
        """
        Yield every context (prefix, suffix) in the given file vector.
        """
        sent_forwards = forward_sentences(file_vector,
                                          context=self.context_length)
        sent_backwards = backward_sentences(file_vector,
                                            context=self.context_length)
        return zip(sent_forwards, sent_backwards)

    def add_prediction(self, name: str, context: bytes,
                       prediction: np.ndarray) -> None:
        """
        Add the prediction (model, context) -> prediction
        """
        assert self._conn

        vector = array.array('f', prediction)
        with self._conn:
            self._conn.execute('BEGIN')
            self._conn.execute(r'''
                INSERT INTO prediction(model, context, data)
                VALUES (:model, :context, :vector)
            ''', dict(model=name, context=context, vector=vector))

    def get_prediction(self, name: str, context: bytes) -> array.array:
        """
        Try to fetch the prediction from the database.

        Returns None if the entry is not found.
        """
        assert self._conn
        cur = self._conn.execute(r'''
            SELECT data
            FROM prediction
            WHERE model = :model AND context = :context
        ''', dict(model=name, context=context))
        result = cur.fetchall()

        if not result:
            # Prediction not found!
            raise KeyError(f'{name}/{context!r}')
        else:
            # Return the precomputed prediction.
            output = array.array('f')
            output.frombytes(result[0][0])
            return output

    def _connect(self, filename: Path) -> sqlite3.Connection:
        # Connect to the database
        conn = sqlite3.connect(str(filename))
        # Initialize the database.
        with conn:
            conn.executescript(self.SCHEMA)
            # Some speed optimizations:
            # http://codificar.com.br/blog/sqlite-optimization-faq/
            conn.executescript(r'''
                PRAGMA journal_mode = WAL;
                PRAGMA synchronous = normal;
            ''')
        return conn



def syntax_ok(filename: str) -> bool:
    """
    Check if the syntax is okay.
    """
    with open(filename, 'rt') as source_file:
        return check_syntax_file(cast(TextIO, source_file))


class Mutations(Sized, Iterable[Tuple[SourceFile, Edit]]):
    """
    Persist every mutation, and enough data to reconstruct every single
    prediction.
    """

    SCHEMA = r"""
    PRAGMA encoding = "UTF-8";

    CREATE TABLE IF NOT EXISTS mutant_with_status (
        hash            TEXT NOT NULL,      -- file hash
        type            TEXT NOT NULL,      -- 'i', 'x', or 's'
        location        INTEGER NOT NULL,   -- location in the file (0-indexed)
        new_token       INTEGER,
        original_token  INTEGER,

        correct         BOOLEAN NOT NULL,  -- Whether the result is valid.

        PRIMARY KEY (hash, type, location, new_token, original_token)
    );

    -- Only syntactically-incorrect mutants.
    CREATE VIEW IF NOT EXISTS mutant
    AS SELECT hash, type, location, original_token, new_token
    WHERE correct = false;

    -- Only syntacticall-correct mutants.
    CREATE VIEW IF NOT EXISTS correct_mutant
    AS SELECT hash, type, location, original_token, new_token
    WHERE correct = true;
    """

    def __init__(self, database: Path=MUTATIONS_PATH,
                 read_only: bool = False) -> None:
        self.program: Optional[SourceFile] = None
        self._dbname = database
        self._conn: Optional[sqlite3.Connection] = None
        self.read_only = read_only

    def __len__(self) -> int:
        assert self._conn
        return self._conn.execute(r'''
            SELECT COUNT(*) FROM mutant
        ''').fetchall()[0][0]

    def __iter__(self) -> Iterator[Tuple[SourceFile, Edit]]:
        assert self._conn
        cur = self._conn.execute(r'''
            SELECT hash, type, location, new_token, original_token
            FROM mutant
        ''')
        for file_hash, code, location, new_token, original_token in cur:
            mutation = Edit.deserialize(code, location, new_token,
                                        original_token)
            yield SourceFile(file_hash), mutation

    @property
    def current_source_hash(self) -> str:
        """
        hash of the current source file being mutated.
        """
        if self.program:
            return self.program.filehash
        else:
            raise ValueError('program not set')

    def add_mutant(self, mutation: Edit) -> None:
        """
        Register a new complete mutation.
        """
        return self._add_mutant(mutation, correct=False)

    def add_correct_file(self, mutation: Edit) -> None:
        """
        Records that a mutation created a syntactically-correct file.
        """
        return self._add_mutant(mutation, correct=True)

    def _add_mutant(self, mutation: Edit, *, correct: bool) -> None:
        assert self._conn

        sql = r'''
            INSERT INTO mutant(
                hash, type, location, new_token, original_token, correct
            ) VALUES (:hash, :type, :location, :new, :original, :correct)
        '''

        code, location, new_token, original_token = mutation.serialize()

        args = dict(hash=self.current_source_hash,
                    type=code, location=location,
                    new=new_token, original_token=original_token,
                    correct=correct)

        with self._conn:
            self._conn.execute(sql, args)

    def __enter__(self) -> 'Mutations':
        # Connect to the database
        conn = self._conn = sqlite3.connect(str(self._dbname))
        # Initialize the database.
        if not self.read_only:
            with conn:
                conn.executescript(self.SCHEMA)
                # Some speed optimizations:
                # http://codificar.com.br/blog/sqlite-optimization-faq/
                conn.executescript(r'''
                    PRAGMA journal_mode = WAL;
                    PRAGMA synchronous = normal;
                ''')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._conn.close()
        self._conn = self._program = None


def test() -> None:
    """
    Test source code mutations.
    """

    # The token stream INCLUDES start and stop tokens.
    # TODO: Fix these tests to make up for 20 token "margin"

    """
    program = SourceCode('DEADBEEF', [0, 86, 5, 31, 99])
    a = Addition.create_random_mutation(program)
    b = Addition.create_random_mutation(program)
    c = Addition.create_random_mutation(program)
    assert len({a, b, c, c, b, a, c, b}) == 3

    d1 = Addition(1, 1)
    d2 = Addition(1, 1)
    s = Substitution(1, 1)
    assert d1 == d2
    assert s != d1

    mutation = Addition.create_random_mutation(program)
    mutation.format(program)

    p = Persistence(database=':memory:')

    original_model = ModelRecipe.from_string('python-f-310-5.4.5.h5')
    alternate_model = ModelRecipe.from_string('python-b-310-5.4.5.h5')
    sentence = (0, 10, 12, 67, 32)
    context = np.array(sentence, np.uint8)
    predictions = np.array([random.random() for _ in range(100)])

    with p:
        # Initially, it should return None.
        pred = p.get_prediction(model_recipe=original_model, context=context)
        assert pred is None

        p.add_prediction(model_recipe=original_model, context=context,
                         prediction=predictions)

        # Get the new prediction.
        result = p.get_prediction(model_recipe=original_model,
                                  context=context)
        assert result is not None
        assert all(x == y for x, y in zip(predictions, result))
        # Ensure the other context is not the same.
        assert p.get_prediction(model_recipe=alternate_model,
                                context=context) is None
    """


def main() -> None:
    # Requires: corpus, model data (backwards and forwards)
    args = parser.parse_args()

    test_set_filename: Path = args.test_set
    limit: int = args.limit
    offset: int = args.offset

    SourceFile.vectors = Vectors.connect_to(VECTORS_PATH)
    SourceFile.corpus = Corpus.connect_to(SOURCES_PATH)

    # Loads the parallel models.
    predictions = Predictions(args.fold)

    # Load the test set. Assume the file hashes are already in random order.
    with open(str(test_set_filename)) as test_set_file:
        test_set = tuple(line.strip() for line in test_set_file
                         if line.strip())

    # Resize the test set
    # TODO: find offset based on file hash
    upper_bound = offset + limit if limit is not None else len(test_set)
    test_set = test_set[offset:upper_bound]

    print(f"Considering {len(test_set)} test files to mutate...")

    # Mutate each file in the test set.
    with Mutations() as persist:
        for file_hash in tqdm(test_set):
            program = SourceFile(file_hash)
            mutate_file(program, persist, predictions, args.mutations)


CONTEXT_LENGTH = 20
SENTENCE_LENGTH = CONTEXT_LENGTH + 1
MIN_LENGTH = SENTENCE_LENGTH * 2

mutation_kinds: Sequence[Type[Edit]] = (Insertion, Deletion, Substitution)


def mutate_file(program: SourceFile,
                persist: Mutations,
                sensibility: Predictions,
                max_mutations_global: int):
    usable_length = len(program.vector) - MIN_LENGTH

    # Ignore files that are too short.
    if usable_length > 0:
        return

    persist.program = program

    progress = tqdm(total=max_mutations_global * len(mutation_kinds),
                    leave=False)
    for mutation_kind in mutation_kinds:
        failures = 0
        mutations_seen: Set[Edit] = set()

        # Clamp down the maximum number of mutations.
        max_mutations = at_most(max_mutations_global, usable_length)
        max_failures = max_mutations

        while failures < max_failures and len(mutations_seen) < max_mutations:
            vector = program.vector
            mutation: Edit = mutation_kind.create_random_mutation(vector)
            if mutation in mutations_seen:
                failures += 1
                continue
            mutations_seen.add(mutation)
            mutant = mutation.apply(vector)

            # Write out the mutated file.
            with NamedTemporaryFile(mode='w+', encoding='UTF-8') as mutant_file:
                # Apply the mutatation and write it to disk.
                mutant.print(file=mutant_file)
                mutant_file.flush()

                # Try the file, reject if it compiles.
                if syntax_ok(mutant_file.name):
                    persist.add_correct_file(mutation)
                    failures += 1
                    continue

                # Do it!
                sensibility.predict(mutant_file.name)

            persist.add_mutant(mutation)
            progress.update(1)

            # Update the description.
            progress.set_description(
                f'{mutation_kind.code} {len(mutations_seen):d}({failures})'
            )

    # Close the tqdm progress bar.
    progress.close()
    # Clear the LRU cache for the new file.
    sensibility.clear_cache()


if __name__ == '__main__':
    main()
