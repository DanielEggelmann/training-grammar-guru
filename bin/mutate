#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

# Copyright 2017 Eddie Antonio Santos <easantos@ualberta.ca>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
Mutate and predict on one fold.

Does not **evaluate**, simply mutates files and performs predictions.

This evaluation:
 - is intended to test the algorithm given a number of different scenarios
 - demonstrates theoretical efficacy
 - is not representative of "real-life" errors

"""

# TODO: Delete all of the stuff replaced by SourceFile, TokenSequence, Edit
# TODO: Create dedicated prediction storage.
# TODO: Create dedicated mutation storage (use Edit#serialize())
# TODO: serialize predictions (context AND result) using array.array.

import argparse
import array
import functools
import io
import random
import sqlite3
import sys
from tempfile import NamedTemporaryFile

from pathlib import Path
from typing import Iterator, Iterable, Optional, Sequence, Set, Sized, Tuple, Type

from tqdm import tqdm
import numpy as np

from sensibility import (
    Vectors, Corpus, Vind, vocabulary,
    SourceFile,
    Edit, Insertion, Deletion, Substitution,
)
from sensibility.model import Model
from sensibility._paths import (
    MODEL_DIR,
    VECTORS_PATH, SOURCES_PATH,
    PREDICTIONS_PATH, MUTATIONS_PATH
)
from sensibility.abram import at_most
from sensibility.tokenize_js import check_syntax_file

# According to Campbell et al. 2014
MAX_MUTATIONS = 120

# Set up the argument parser
parser = argparse.ArgumentParser()
parser.add_argument('fold', type=int)
parser.add_argument('test_set', type=Path)
parser.add_argument('-k', '--mutations', type=int, default=MAX_MUTATIONS)
parser.add_argument('-n', '--limit', type=int, default=None)
parser.add_argument('-o', '--offset', type=int, default=0)

# Schema for the results database.
SCHEMA = r"""
PRAGMA encoding = "UTF-8";

CREATE TABLE IF NOT EXISTS mutant (
    hash        TEXT NOT NULL,  -- file hash
    type        TEXT NOT NULL,  -- 'addition', 'deletion', or 'substitution'
    location    INTEGER,        -- location in the file (0-indexed)
    token       INTEGER,        -- addition: the token inserted
                                -- deletion: the token deleted
                                -- substitution: the token that has replaced the old token

    PRIMARY KEY (hash, type, location, token)
);

CREATE TABLE IF NOT EXISTS prediction (
    model   TEXT NOT NULL,      -- model that created the prediction
    context BLOB NOT NULL,      -- input of the prediction

    data    BLOB NOT NULL,      -- prediction data, as a numpy array

    PRIMARY KEY (model, context)
);

-- same as `mutant`, but contains syntactically-correct mutants.
CREATE TABLE IF NOT EXISTS correct_mutant (
    hash        TEXT NOT NULL,
    type        TEXT NOT NULL,
    location    INTEGER,
    token       INTEGER,

    PRIMARY KEY (hash, type, location, token)
);
"""


class Predictions:
    """
    Stores predictions.
    """
    def __init__(self, fold: int, filename: Path=PREDICTIONS_PATH) -> None:
        self.forwards_model = Model(MODEL_DIR / f"javascript-f{fold}.hdf5")
        self.forwards_model = Model(MODEL_DIR / f"javascript-b{fold}.hdf5",
                                    backwards=True)
        # XXX: Hard code the context length!
        self.context_length = 20
        self._conn = sqlite3.connect(':memory:')

        def _predict(model_recipe, model, tuple_context):
            """
            Does prediction, consulting the database first before consulting
            the model.
            """
            context = array.array(tuple_context, np.uint8)
            stashed_prediction = self.persistence.get_prediction(
                    model_recipe=model_recipe,
                    context=context
            )
            if stashed_prediction is None:
                prediction = model.predict(context)
                self.persistence.add_prediction(
                        model_recipe=model_recipe,
                        context=context,
                        prediction=prediction)
                return prediction
            return stashed_prediction

        # Create cached prediction functions.
        @functools.lru_cache(maxsize=2**16)
        def predict_forwards(prefix):
            return _predict(forwards, self.forwards_model, prefix)

        @functools.lru_cache(maxsize=2**16)
        def predict_backwards(suffix):
            return _predict(backwards, self.backwards_model, suffix)

        self.predict_forwards = predict_forwards
        self.predict_backwards = predict_backwards

    def predict(self, filename):
        """
        Predicts at each position in the file.

        Side-effect: writes predictions to persistence.
        """

        # Get file vector for this (incorrect) file.
        with open(str(filename), 'rt', encoding='UTF-8') as script:
            tokens = tokenize_file(script)
        file_vector = vectorize_tokens(tokens)

        # Create predictions.
        for (prefix, _), (suffix, _) in self.contexts(file_vector):
            self.predict_forwards(tuple(prefix))
            self.predict_backwards(tuple(suffix))

    def clear_cache(self):
        self.predict_forwards.cache_clear()
        self.predict_backwards.cache_clear()

    def cache_info(self):
        return self.predict_forwards.cache_info(), self.predict_backwards.cache_info()

    def contexts(self, file_vector):
        """
        Yield every context (prefix, suffix) in the given file vector.
        """
        sent_forwards = Sentences(file_vector,
                                  size=self.context_length,
                                  backwards=False)
        sent_backwards = Sentences(file_vector,
                                   size=self.context_length,
                                   backwards=True)
        return zip(sent_forwards, chop_prefix(sent_backwards))

    def add_prediction(self, *, context: Sequence[Vind],
                       prediction: np.ndarray) -> None:
        """
        Add the prediction (model, context) -> prediction
        """
        assert self._conn

        with self._conn:
            self._conn.execute('BEGIN')
            self._conn.execute(r'''
                INSERT INTO prediction(model, context, data)
                VALUES (:model, :context, :data)
            ''', dict(context=serialize_context(context),
                      data=prediction))

    def get_prediction(self, *, model_recipe=None, context=None):
        """
        Try to fetch the prediction from the database.

        Returns None if the entry is not found.
        """
        assert self._conn
        assert isinstance(model_recipe, ModelRecipe)
        cur = self._conn.execute(r'''
            SELECT data
            FROM prediction
            WHERE model = :model AND context = :context
        ''', dict(model=model_recipe.identifier,
                  context=serialize_context(context)))
        result = cur.fetchall()

        if not result:
            # Prediction not found!
            return None
        else:
            # Return the precomputed prediction.
            return unblob(result[0][0])


def syntax_ok(filename):
    """
    Check if the syntax is okay.
    """
    with open(filename, 'rb') as source_file:
        return check_syntax_file(source_file)


def serialize_context(context: Sequence[Vind]) -> array.array:
    """
    Convert context to an unsigned 8-bit array.array.

    >>> serialize_context([32, 67, 23, 99])
    array('B', [32, 67, 23, 99])
    """
    return array.array('B', context)


class Mutations(Sized, Iterable[Tuple[SourceFile, Edit]]):
    """
    Persist every mutation, and enough data to reconstruct every single
    prediction.
    """

    SCHEMA = r"""
    PRAGMA encoding = "UTF-8";

    CREATE TABLE IF NOT EXISTS mutant_with_status (
        hash            TEXT NOT NULL,      -- file hash
        type            TEXT NOT NULL,      -- 'i', 'x', or 's'
        location        INTEGER NOT NULL,   -- location in the file (0-indexed)
        new_token       INTEGER,
        original_token  INTEGER,

        correct         BOOLEAN NOT NULL,  -- Whether the result is valid.

        PRIMARY KEY (hash, type, location, new_token, original_token)
    );

    -- Only syntactically-incorrect mutants.
    CREATE VIEW IF NOT EXISTS mutant
    AS SELECT hash, type, location, original_token, new_token
    WHERE correct = false;

    -- Only syntacticall-correct mutants.
    CREATE VIEW IF NOT EXISTS correct_mutant
    AS SELECT hash, type, location, original_token, new_token
    WHERE correct = true;
    """

    def __init__(self, database: Path=MUTATIONS_PATH,
                 read_only: bool = False) -> None:
        self.program: Optional[SourceFile] = None
        self._dbname = database
        self._conn: Optional[sqlite3.Connection] = None
        self.read_only = read_only

    def __len__(self) -> int:
        assert self._conn
        return self._conn.execute(r'''
            SELECT COUNT(*) FROM mutant
        ''').fetchall()[0][0]

    def __iter__(self) -> Iterator[Tuple[SourceFile, Edit]]:
        assert self._conn
        cur = self._conn.execute(r'''
            SELECT hash, type, location, new_token, original_token
            FROM mutant
        ''')
        for file_hash, code, location, new_token, original_token in cur:
            mutation = Edit.deserialize(code, location, new_token,
                                        original_token)
            yield SourceFile(file_hash), mutation

    @property
    def current_source_hash(self) -> str:
        """
        hash of the current source file being mutated.
        """
        if self.program:
            return self.program.filehash
        else:
            raise ValueError('program not set')

    def add_mutant(self, mutation: Edit) -> None:
        """
        Register a new complete mutation.
        """
        return self._add_mutant(mutation, correct=False)

    def add_correct_file(self, mutation: Edit) -> None:
        """
        Records that a mutation created a syntactically-correct file.
        """
        return self._add_mutant(mutation, correct=True)

    def _add_mutant(self, mutation: Edit, *, correct: bool) -> None:
        assert self._conn

        sql = r'''
            INSERT INTO mutant(
                hash, type, location, new_token, original_token, correct
            ) VALUES (:hash, :type, :location, :new, :original, :correct)
        '''

        code, location, new_token, original_token = mutation.serialize()

        args = dict(hash=self.current_source_hash,
                    type=code, location=location,
                    new=new_token, original_token=original_token,
                    correct=correct)

        with self._conn:
            self._conn.execute(sql, args)

    def __enter__(self) -> 'Mutations':
        # Connect to the database
        conn = self._conn = sqlite3.connect(str(self._dbname))
        # Initialize the database.
        if not self.read_only:
            with conn:
                conn.executescript(self.SCHEMA)
                # Some speed optimizations:
                # http://codificar.com.br/blog/sqlite-optimization-faq/
                conn.executescript(r'''
                    PRAGMA journal_mode = WAL;
                    PRAGMA synchronous = normal;
                ''')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._conn.close()
        self._conn = self._program = None


def test() -> None:
    """
    Test source code mutations.
    """

    # The token stream INCLUDES start and stop tokens.
    # TODO: Fix these tests to make up for 20 token "margin"

    """
    program = SourceCode('DEADBEEF', [0, 86, 5, 31, 99])
    a = Addition.create_random_mutation(program)
    b = Addition.create_random_mutation(program)
    c = Addition.create_random_mutation(program)
    assert len({a, b, c, c, b, a, c, b}) == 3

    d1 = Addition(1, 1)
    d2 = Addition(1, 1)
    s = Substitution(1, 1)
    assert d1 == d2
    assert s != d1

    mutation = Addition.create_random_mutation(program)
    mutation.format(program)

    p = Persistence(database=':memory:')

    original_model = ModelRecipe.from_string('python-f-310-5.4.5.h5')
    alternate_model = ModelRecipe.from_string('python-b-310-5.4.5.h5')
    sentence = (0, 10, 12, 67, 32)
    context = np.array(sentence, np.uint8)
    predictions = np.array([random.random() for _ in range(100)])

    with p:
        # Initially, it should return None.
        pred = p.get_prediction(model_recipe=original_model, context=context)
        assert pred is None

        p.add_prediction(model_recipe=original_model, context=context,
                         prediction=predictions)

        # Get the new prediction.
        result = p.get_prediction(model_recipe=original_model,
                                  context=context)
        assert result is not None
        assert all(x == y for x, y in zip(predictions, result))
        # Ensure the other context is not the same.
        assert p.get_prediction(model_recipe=alternate_model,
                                context=context) is None
    """


def main() -> None:
    # Requires: corpus, model data (backwards and forwards)
    args = parser.parse_args()

    test_set_filename: Path = args.test_set
    limit: int = args.limit
    offset: int = args.offset

    SourceFile.vectors = Vectors.connect_to(VECTORS_PATH)
    SourceFile.corpus = Corpus.connect_to(SOURCES_PATH)

    # Loads the parallel models.
    predictions = Predictions(args.fold)

    # Load the test set. Assume the file hashes are already in random order.
    with open(str(test_set_filename)) as test_set_file:
        test_set = tuple(line.strip() for line in test_set_file
                         if line.strip())

    # Resize the test set
    upper_bound = offset + limit if limit is not None else len(test_set)
    test_set = test_set[offset:upper_bound]

    print(f"Considering {len(test_set)} test files to mutate...")

    # Mutate each file in the test set.
    with Mutations() as persist:
        for file_hash in tqdm(test_set):
            program = SourceFile(file_hash)
            mutate_file(program, persist, predictions, args.mutations)

CONTEXT_LENGTH = 20
SENTENCE_LENGTH = CONTEXT_LENGTH + 1
MIN_LENGTH = SENTENCE_LENGTH * 2

mutation_kinds: Sequence[Type[Edit]] = (Insertion, Deletion, Substitution)


def mutate_file(program: SourceFile,
                persist: Mutations,
                sensibility: Predictions,
                max_mutations_global: int):
    usable_length = len(program.vector) - MIN_LENGTH

    # Ignore files that are too short.
    if usable_length > 0:
        return

    persist.program = program

    progress = tqdm(total=max_mutations_global * len(mutation_kinds),
                    leave=False)
    for mutation_kind in mutation_kinds:
        failures = 0
        mutations_seen: Set[Edit] = set()

        # Clamp down the maximum number of mutations.
        max_mutations = at_most(max_mutations_global, usable_length)
        max_failures = max_mutations

        while failures < max_failures and len(mutations_seen) < max_mutations:
            vector = program.vector
            mutation: Edit = mutation_kind.create_random_mutation(vector)
            if mutation in mutations_seen:
                failures += 1
                continue
            mutations_seen.add(mutation)
            mutant = mutation.apply(vector)

            # Write out the mutated file.
            with NamedTemporaryFile(mode='w+', encoding='UTF-8') as mutant_file:
                # Apply the mutatation and write it to disk.
                mutant.print(file=mutant_file)
                mutant_file.flush()

                # Try the file, reject if it compiles.
                if syntax_ok(mutant_file):
                    persist.add_correct_file(mutation)
                    failures += 1
                    continue

                # Do it!
                sensibility.predict(mutant_file.name)

            persist.add_mutant(mutation)
            progress.update(1)

            # Update the description.
            progress.set_description(
                f'{mutation_kind.code} {len(mutations_seen):d}({failures})'
            )

    # Close the tqdm progress bar.
    progress.close()
    # Clear the LRU cache for the new file.
    sensibility.clear_cache()


if __name__ == '__main__':
    main()
