#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

# Copyright 2016 Eddie Antonio Santos <easantos@ualberta.ca>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import re
import sys
import argparse
from contextlib import redirect_stderr
from typing import Optional, Any, Sequence, Iterator, Iterable, Tuple

from blessings import Terminal

from sensibility import (
    Sensibility, Token, Edit, Corpus, Vectors, SourceFile, SourceVector,
    vocabulary, Vind
)
from sensibility.fix import IndexResult, temporary_program
from sensibility.tokenize_js import tokenize
from sensibility._paths import VECTORS_PATH, SOURCES_PATH


t = Terminal()

parser = argparse.ArgumentParser()
subparsers = parser.add_subparsers()

parser_ins = subparsers.add_parser('insert')
parser_ins.add_argument('location', type=int)
parser_ins.add_argument('token', type=str)
parser_ins.set_defaults(kind='i')

parser_del = subparsers.add_parser('delete')
parser_del.add_argument('location', type=int)
parser_del.set_defaults(kind='x')

parser_sub = subparsers.add_parser('substitute')
parser_sub.add_argument('location', type=int)
parser_sub.add_argument('token', type=str)
parser_sub.set_defaults(kind='s')


parser.add_argument('filehash', type=str)


def status(*message):
    print('\r', end='')
    print(t.yellow, *message, t.normal, end='')
    sys.stdout.flush()


def to_vind(text: str) -> Optional[Vind]:
    if len(text) == 0:
        return None
    else:
        return vocabulary.to_index(text)


def get_edit(args: Any, source: SourceVector) -> Edit:
    kind = args.kind
    new = ''

    if kind == 'i':
        new = args.token
    elif kind == 's':
        new = args.token

    original = source[args.location]
    new_token = to_vind(new)
    return Edit.deserialize(kind, args.location, new_token, original)


def lines(tokens: Sequence[Token]) -> Iterator[
        Tuple[int, Sequence[Tuple[int, Token]]]
]:
    i = 0
    while i < len(tokens):
        current_line = tokens[i].line
        j = i
        while j < len(tokens):
            if tokens[j].line != current_line:
                break
            j += 1
        yield current_line, tuple(enumerate(tokens[i:j], start=i))
        i = j


def format_token(token: Token, index: int) -> str:
    text = token.value
    result = index2result.get(index, None)
    if result is None:
        return f'{t.bold}{text}{t.normal}'
    steps = 0xFF - 0xE8
    color = 0xE8 + int(steps * (1 - float(result)))
    return f"\033[38;5;{color}m{text}{t.normal}"


if __name__ == '__main__':
    args = parser.parse_args()
    filehash: str  = args.filehash

    corpus = SourceFile.corpus = Corpus.connect_to(SOURCES_PATH)
    SourceFile.vectors = Vectors.connect_to(VECTORS_PATH)

    status(f"Fetching info about {filehash}...")
    repo, owner, path = corpus.file_info(filehash)
    path = re.sub('^[^/]+?/', '', path)

    original_file = SourceFile(filehash)
    edit = get_edit(args, original_file.vector)

    print(f"Applying {edit} to {filehash}")
    print(f"https://github.com/{repo}/{owner}/blob/master/{path}")


    mutant = edit.apply(original_file.vector)

    status("Initializing models...")
    with open('/dev/null', 'w') as null, redirect_stderr(null):
        sensibility = Sensibility(0)

    with temporary_program(mutant) as mutated_file:
        # Do the (canned) prediction...
        status(f"Predicting on {mutated_file.name}")
        ranked_locations, fixes = sensibility.rank_and_fix(mutated_file.name)

    print(fixes)

    index2result = {loc.index: loc for loc in ranked_locations}

    # Print the file.
    for line_no, line in lines(original_file.source_tokens):
        # Print the initial indent, if any
        initial_indent =  ' ' * line[0][1].column
        print(f"{line_no:4d}: {initial_indent}", end='')
        for index, token in line:
            if index == edit.index:
                value = vocabulary.to_text(edit.serialize()[2] or Vind(0))
                result = float(index2result[index])
                print(f"{t.bold_red}{value}{t.normal}",
                      f"〘{t.bold}{result:.2f}{t.normal}〙",
                      end=' ')
            print(format_token(token, index), end=' ')
        print()
